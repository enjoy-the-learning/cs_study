# 문제

1. 프록시와 리버스 프록시의 차이는 무엇인가요?
2. 리버스 프록시에서 로드 밸런싱이 필요한 이유는 무엇이며, 어떤 알고리즘들이 사용될 수 있나요?
3. API 게이트웨이와 리버스 프록시는 모두 중계 역할을 합니다. 두 개념의 차이점을 설명하고, 각각을 사용하기 적절한 상황을 비교해주세요
4. 고가용성을 위해 리버스 프록시 서버에 어떤 구조를 적용할 수 있나요? 실제 구성 방법 예시와 함께 설명해주세요
5. 스케일 업과 스케일 아웃의 차이를 설명하고, 오토스케일링이 실무에서 어떤 상황에서 효과적인지 설명해주세요

---

## 프록시 (Forward Proxy)

클라이언트와 서버 사이에 위치하여,
클라이언트가 서버에 직접 요청하지 않고 **프록시 서버를 통해 간접적으로 요청**하는 방식

> **클라이언트의 대리인**
클라이언트 입장에서 “프록시 서버가 나 대신 요청을 보내준다”
> 

### 동작 방식

1. 사용자가 웹사이트 접속을 요청
2. 프록시 서버가 이 요청을 받아, 대신 웹사이트에 요청
3. 웹사이트에서 응답을 받고, 프록시가 이를 사용자에게 전달

### 주요 용도

- **사용자 보호**
    - 프록시 사용자가 외부와 직접 통신하지 않고 대신 요청을 보내는 중간 서버 역할
    
    | 보호 요소 | 설명 |
    | --- | --- |
    | **IP 주소 은닉** | 서버는 클라이언트의 실제 IP를 보지 못하고, 프록시 IP만 확인 |
    | **접근 제어** | 사용자가 접속할 수 있는 웹사이트를 제한함 (관리자 정책 반영 가능) |
    | **익명성** | 사용자의 위치, 브라우저 정보 등을 숨기거나 가림 |
    | **콘텐츠 필터링** | 악성 웹사이트 접근 차단 등 보안 강화 |
    | **캐싱 제공** | 외부 사이트에 반복 요청을 안 해도 됨 → 더 빠르고 안전한 연결 가능 |

### 예시

- 회사나 학교에서 인터넷 사용을 관리하는 **웹 프록시 서버**
- Tor, VPN 등도 일종의 프록시 기능
    - Tor는 다단계 프록시로 구성돼 사용자 추적이 어려움

---

## 리버스 프록시 (Reverse Proxy)

서버 앞단에서 동작하는 프록시이며,
클라이언트 요청을 받아서 **알맞은 내부 서버로 전달**

> **서버의 대리인**
서버 입장에서 “클라이언트는 나를 통해서만 접근해야 해”
> 

### 동작 방식

1. 클라이언트가 리버스 프록시 서버에 요청
2. 리버스 프록시가 요청을 적절한 백엔드 서버로 전달
3. 백엔드 서버에서 결과를 리버스 프록시로 보내고, 다시 클라이언트에 응답

### 주요 용도

- **서버 보호**
    - 클라이언트가 서버에 직접 접근하는 것을 막고,
    - 프록시 서버가 외부 요청을 받아 적절한 백엔드 서버에 전달
        
        → 외부 요청은 항상 리버스 프록시만 거치므로, 실제 서버 은닉
        
- **분산**
    - 여러 개의 백엔드 서버가 있을 때,
        - 리버스 프록시가 트래픽을 분산 (로드 밸런싱)
        - 서버의 상태/부하에 따라 효율적인 요청 분배

| 기능 | 설명 |
| --- | --- |
| **IP 마스킹** | 백엔드 서버 IP는 외부에 노출되지 않음 |
| **DDoS 완화** | 요청을 필터링하고, 캐시 제공 |
| **TLS 종료** | HTTPS 처리를 리버스 프록시가 맡아, 백엔드는 부담 없음 |
| **로드 밸런싱** | 다수 서버로 요청 분산 (round-robin, least connection 등) |
| **헬스체크** | 죽은 서버로 요청 안 보내도록 감시 기능 내장 가능 |

### 예시

- **Nginx**나 **Apache HTTP Server**에서 리버스 프록시 설정
- AWS CloudFront 같은 CDN(Content Delivery Network)도 일종의 리버스 프록시
    - 단순히 파일 저장소가 아닌,
        - 클라이언트 요청을 대신 받고
        - 캐싱 여부를 판단하고
        - 백엔드로 요청을 전달하거나 자체 응답을 주는 리버스 프록시처럼 동작
            
            
            | 리버스 프록시 기능 | CDN에서의 역할 |
            | --- | --- |
            | 클라이언트 → 프록시 → 서버 | 사용자의 요청을 CDN이 먼저 받고, 원 서버에 대신 요청 |
            | 응답 캐싱 | HTML, JS, CSS, 이미지 등을 저장해둠 |
            | 서버 IP 숨김 | 클라이언트는 실제 서버 주소를 모름 (DDoS 방지) |
            | SSL 종료 | HTTPS 인증서를 CDN에서 처리 (예: Let’s Encrypt 제공) |
            | 로드 밸런싱 | 트래픽을 여러 원 서버로 분산 가능 |
            | 접근 제어/보안 정책 | 봇 차단, 방화벽, Rate Limiting 등 |
        - **CloudFront 흐름**
            
            클라이언트가 `https://cdn.example.com/logo.png` 를 요청할 때:
            
            1. CloudFront 엣지 로케이션(캐시 서버)이 요청을 먼저 받고,
            2. 해당 콘텐츠가 **엣지에 캐싱되어 있다면**,
                
                → CloudFront가 직접 응답 (원 서버 접근 없음)
                
            3. **캐시에 없다면**,
                
                → CloudFront가 설정된 origin 서버에 요청을 보냅니다.
                
                - 오리진은 S3 버킷, EC2, ALB, 또는 외부 도메인일 수도 있음
            4. 오리진 서버에서 응답을 받고,
                
                → CloudFront는 이 응답을 캐시에 저장한 뒤 클라이언트에 전달
                
            
        - **위의 과정을 리버스 프록시 흐름으로 하면,,**
            - 사용자는 오리진 서버에 직접 접근하지 않음
            - 오리진 IP는 외부에 노출되지 않음
            - CloudFront가 **중간에 개입해서 요청을 전달하거나 직접 응답**

---

## API 게이트웨이 (API Gateway)

네트워크 또는 시스템 간 **프로토콜 변환, 요청 라우팅, 보안 기능 등 복합적인 중계 역할**을 함
프록시보다 더 포괄적이고 복잡한 기능을 수행

> 이질적인 시스템이나 네트워크를 이어주는 번역기 역할
> 

### 동작 방식

- 클라이언트 요청을 받아서, API 변환 / 인증 / 라우팅 등을 처리한 후 백엔드 서비스에 전달
- 다양한 서비스 요청을 통합해서 관리

### 주요 용도

- **API 제어**
    - 일반적인 트래픽 라우팅만 하는 게 아니고 API 수준에서 요청을 분석하고 처리하는 애플리케이션 레벨의 관문
    - 단순히 트래픽 중계가 아니라 아래의 판단을 내려야 함
        - 유효한 요청인가?
        - 속도 제한이 필요한가?
        - 인증이 됐는가?
        - 어느 서비스로 보내야 하는가?
        
        | 정책/제어 항목 | 설명 |
        | --- | --- |
        | **인증/인가** | JWT, OAuth, API 키 등을 통해 사용자 권한 확인 |
        | **속도 제한** | abuse 방지 (초당 10회 이상 요청 차단 등) |
        | **라우팅** | URL에 따라 요청을 다른 마이크로서비스로 분배 |
        | **요청/응답 변환** | 필요 시 JSON 구조 변형, 헤더 추가 등 |
        | **로깅/모니터링** | 트래픽 분석, APM, 추적 ID 관리 |
        | **캐싱** | 동일 응답을 저장해 처리 속도 향상 |

### 예시

- **API Gateway**: AWS API Gateway, Kong
    - Kong: 마이크로서비스 아키텍처에서 많이 쓰이는 API Gateway 플랫폼
- 실제 운영 서비스에서 하나의 진입점으로 모든 API를 통제함

---

## 비교 정리

| 항목 | 프록시 (Proxy) | 리버스 프록시 (Reverse Proxy) | 게이트웨이 (Gateway) |
| --- | --- | --- | --- |
| 위치 | 클라이언트 앞단 | 서버 앞단 | 네트워크 또는 API 관문 |
| 주요 대상 | 클라이언트 보호 | 서버 보호 및 분산 | 시스템 간 중계 |
| 주요 목적 | IP 은닉, 캐싱, 필터링 | 로드밸런싱, 보안, SSL 종료 | 인증, 라우팅, 프로토콜 변환 |
| 예시 | VPN, 웹 프록시 | Nginx 리버스 프록시 | AWS API Gateway, Kong |
| 사용 맥락 | 일반 사용자 → 외부 | 외부 → 내부 서버 | 마이크로서비스, 하이브리드 시스템 |

---

서비스가 장애 없이 동작하려면 트래픽을 중계하는 구성 요소(프록시, 리버스 프록시, API 게이트웨이 등)가 SPOF가 되면 안 됨

- **SPOF:** Single Point of Failure(단일 실패 지점)
- 시스템의 구성 요소 중 하나가 장애를 일으키면 전체 시스템이 중단되는 것

이때 필요한 개념이 고가용성과 이중화인데,

## 고가용성 (High Availability)

장애가 발생해도 서비스가 지속적으로 운영될 수 있는 시스템의 특성을 말함

→ 목표는 서비스 연속성 보장

### 구성 요소

- **Failover:** 특정 인스턴스에 장애가 발생했을 때, 트래픽을 자동으로 정상적인 다른 인스턴스로 전환하여 서비스 중단을 방지
- **헬스 체크(Health Check):** 시스템 또는 인스턴스의 상태를 주기적으로 점검하여, 비정상 상태인 노드를 로드 밸런싱 대상에서 제외
- **로드 밸런싱(Load Balancing):** 트래픽을 여러 인스턴스에 분산시켜 부하를 분산하고 일부 장애에도 서비스가 유지되도록 함
- **이중화(Redundancy):** 동일한 구성 요소를 복수로 준비하여 단일 장애 지점을 제거

### 예시

- 여러 리버스 프록시 인스턴스를 구성하고 로드 밸런서를 앞단에 둠
    
    → 한 대의 프록시에 장애가 발생하면 로드 밸런서가 다른 정상 인스턴스로 트래픽을 전달함 (헬스 체크로 어떤 프록시가 살아있는지 확인)
    
- API 게이트웨이를 다중 AZ(가용 영역)으로 배포
    
    → 한 AZ에서 장애가 발생해도 다른 AZ에 있는 인스턴스가 서비스 제공
    

---

## 이중화 (Redundancy)

특정 구성 요소가 장애를 일으켜도 대체 가능한 동일한 구성 요소를 준비해두는 것을 말함

### 방식

- **하드웨어 이중화:** 서버나 네트워크 장비를 2대 이상 구성하여 장애 발생 시 즉시 대체 가능하도록 함
- **소프트웨어 이중화:** 프로세스를 중복 실행하거나, 마이크로서비스 형태로 분산시켜 서비스 중단을 방지
- **네트워크 이중화:** ISP, 스위치, 라우터 등 네트워크 경로를 이중으로 구성하여 단일 경로 장애에 대비

### 예시

- 두 개의 Nginx 서버를 구성하고, keepalived를 이용해 장애 발생 시 자동 전환
    - **keepalived:** 가상 IP를 이용해 장애가 생겼을 때 자동으로 다른 서버로 트래픽을 넘겨주는 리눅스 기반 고가용성 도구
    1. **초기 상태:**
        - Nginx 서버 A가 Main, B는 백업
        - Virtual IP는 A에 바인딩되어 있음
    2. **장애 발생 시:**
        - A가 다운되면, keepalived가 이를 감지
        - Virtual IP가 자동으로 B로 이동 (Failover)
        - 클라이언트는 계속 같은 IP로 접속하지만, 실제 요청은 이제 B가 처리함
    3. **A 복구 시 (선택 설정):**
        - A가 복구되면 다시 Main으로 전환되게 할 수도 있고,
        - 그대로 B가 Main 역할을 유지하도록 설정할 수도 있음

---

## 로드 밸런싱 (Load Balancing)

고가용성과 이중화 구조에서 로드 밸런서(Load Balancer)는 핵심 컴포넌트

- L4(TCP/UDP) 혹은 L7(HTTP) 계층에서 동작
- 클라이언트 요청을 헬스 체크 기반으로 다수의 서버에 분산
- 장애가 감지된 서버는 자동으로 대상에서 제외되어 트래픽이 다른 서버로 전달됨 (Failover)

### 로드 밸런싱 알고리즘

- **라운드 로빈(Round Robin):** 서버 리스트를 순차적으로 순회하며 요청을 분산
- **가중 라운드 로빈(Weighted Round Robin):** 각 서버에 가중치를 부여해 더 높은 처리 능력을 가진 서버에 요청을 더 많이 분배 (각 서버의 처리 능력을 반영함)
    - 서버 성능이 모두 비슷하다면 RR, 아니라면 WRR
- **최소 연결(Least Connections):** 현재 연결 수가 가장 적은 서버로 요청을 전달
- **IP 해시(IP Hash):** 클라이언트 IP 기반으로 특정 서버에 항상 연결되도록 함 (세션 고정)

---

## 스케일링 (Scaling)

서비스 성능을 향상시키기 위한 인프라 확장 방법

- **스케일 업 (Scale-Up):**
    
    하나의 서버 성능을 높이는 방식 (CPU, 메모리, 디스크 등 업그레이드)
    
    → 구조 변경은 적지만, 확장성에는 한계가 있음 (일정 수준 이상으로는 업그레이드 불가능)
    
- **스케일 아웃 (Scale-Out):**
    
    서버 수를 수평으로 늘리는 방식 (여러 대를 병렬로 운용)
    
    → 장애에 강하고 유연한 확장이 가능
    

일반적으로 스케일 아웃이 더 안정적이고 확장성이 뛰어나다~

- 특정 서버가 장애를 일으켜도 전체 서비스에는 영향이 적고,
- 비용과 장애 대비 효율 면에서도 유리하기 때문

## 오토스케일링 (Auto Scaling)

스케일 아웃과 함께 사용되는 자동화된 확장 메커니즘인데,

- 트래픽 변화에 따라 서버 수를 자동으로 늘리거나 줄이는 기능임
    - 갑작스러운 트래픽 급증 시 자동으로 서버 인스턴스를 추가
    - **예시:** 수강신청, 콘서트 예매, 쇼핑몰 할인 이벤트 등
- 클라우드 환경에서 고가용성과 비용 효율성을 동시에 확보 가능
- AWS, GCP, Azure 같은 클라우드 서비스에서는 오토스케일링 기능을 기본적으로 지원

---
